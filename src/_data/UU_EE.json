{
  "title": "Empirical Economics",
  "shorttitle": "UU Empirical Econ",
  "questions": [
    {
      "id": "mcq_006",
      "questionText": "Log–level model: $\\log(y) = \\alpha + \\beta x$ with \\beta = 0.08$. Best interpretation?",
      "options": [
        "Causal inference",
        "Prediction",
        "Policy evaluation",
        "Hypothesis testing"
      ],
      "correctAnswer": "Prediction",
      "difficulty": "easy",
      "topic": "Lecture 1 – Prediction vs Causation",
      "explanation": "Prediction prioritizes minimizing forecast error on unseen data (e.g., rolling or expanding window RMSE). Causal inference would target an unbiased effect under identification assumptions like $E[u|X]=0$."
    },
    {
      "id": "mcq_002",
      "questionText": "A survey samples different households in 2018 and a new independent sample in 2024 (no tracking of the same households). What type of data is this?",
      "options": [
        "Cross-sectional data",
        "Time series data",
        "Pooled cross-sections",
        "Panel (longitudinal) data"
      ],
      "correctAnswer": "Pooled cross-sections",
      "difficulty": "medium",
      "topic": "Lecture 1 – Data Types",
      "explanation": "Two independent cross-sections at different times form pooled cross-sections. Panel data would follow the same units; a time series is one variable over time."
    },
    {
      "id": "mcq_003",
      "questionText": "In the OLS model $y = \\beta_0 + \\beta_1 x + u$, which sample property holds by the first-order conditions?",
      "options": [
        "Residuals are uncorrelated with $x$",
        "Residuals are normally distributed",
        "Residual variance is constant",
        "Residuals sum to the mean of $y$"
      ],
      "correctAnswer": "Residuals are uncorrelated with $x$",
      "difficulty": "medium",
      "topic": "Lecture 1 – OLS Properties",
      "explanation": "Normal equations imply $\\sum_i \\hat u_i x_i=0$ and (with an intercept) $\\sum_i \\hat u_i=0$. Normality and homoskedasticity are not guaranteed by OLS."
    },
    {
      "id": "mcq_004",
      "questionText": "A simple regression of CEO salary on profits yields $R^2 = 0.09$. Correct interpretation?",
      "options": [
        "9% chance the relation is causal",
        "9% of the variation in salary is explained by profits",
        "Salary rises 9% for each €1 profit",
        "The slope is biased by 9%"
      ],
      "correctAnswer": "9% of the variation in salary is explained by profits",
      "difficulty": "easy",
      "topic": "Lecture 1 – Model Fit",
      "explanation": "$R^2$ is the share of sample variance in $y$ explained by the regressors. It does not imply causality or a percentage slope."
    },
    {
      "id": "mcq_005",
      "questionText": "Log–log model: $\\log(\\text{Sales}) = 3.8 + 0.45\\,\\log(\\text{AdSpend})$. Interpretation of 0.45?",
      "options": [
        "€1 more AdSpend raises sales by 0.45 units",
        "1% more AdSpend is associated with 0.45% more sales",
        "1% more AdSpend raises sales by 0.45 units",
        "Elasticity of log-sales w.r.t. AdSpend in euros"
      ],
      "correctAnswer": "1% more AdSpend is associated with 0.45% more sales",
      "difficulty": "easy",
      "topic": "Lecture 1 – Functional Forms",
      "explanation": "In a log–log model the slope is an elasticity: $\\partial\\ln y/\\partial\\ln x=0.45$. A 1% rise in $x$ is associated with ~0.45% rise in $y$."
    },
    {
      "id": "mcq_006",
      "questionText": "Log–level model: $\\log(y) = a + \\beta x$ with $\\beta = 0.08$. Best interpretation?",
      "options": [
        "A one-unit increase in $x$ changes $y$ by 0.08 units",
        "A one-unit increase in $x$ is associated with an 8% increase in $y$",
        "A 1% increase in $x$ raises $y$ by 0.08%",
        "A one-unit increase in $x$ is associated with a 0.08% increase in $y$"
      ],
      "correctAnswer": "A one-unit increase in $x$ is associated with an 8% increase in $y$",
      "difficulty": "medium",
      "topic": "Lecture 1 – Functional Forms",
      "explanation": "In log–level, $\\partial \\ln y/\\partial x=\\beta$. A one-unit increase in $x$ implies approximately $100\\times \\beta=8\\%$ increase in $y$."
    },
    {
      "id": "mcq_007",
      "questionText": "Regress Wage on Education but omit Ability. Ability is positively correlated with Education and positively affects Wage. Direction of bias on Education?",
      "options": [
        "Upward bias",
        "Downward bias",
        "No bias",
        "Cannot be determined"
      ],
      "correctAnswer": "Upward bias",
      "difficulty": "hard",
      "topic": "Lecture 2 – Omitted Variable Bias",
      "explanation": "Bias sign: $\\operatorname{sign}(\\beta_Z)\\times\\operatorname{sign}(\\operatorname{Cov}(X,Z))=(+)\\times(+)\\Rightarrow$ upward bias in the education coefficient."
    },
    {
      "id": "mcq_008",
      "questionText": "Which is a bad control when estimating the effect of education on wages?",
      "options": [
        "Years of experience (confounder)",
        "Innate ability (confounder)",
        "Current occupation if partly determined by education (mediator)",
        "Age in years"
      ],
      "correctAnswer": "Current occupation if partly determined by education (mediator)",
      "difficulty": "medium",
      "topic": "Lecture 2 – Good vs Bad Controls",
      "explanation": "Controlling for a mediator blocks part of the treatment effect and can induce bias. Confounders (pre-treatment) are appropriate controls."
    },
    {
      "id": "mcq_009",
      "questionText": "Which change generally makes the OLS slope more precise?",
      "options": [
        "Reduce sample size",
        "Increase error variance",
        "Decrease variance of $x$",
        "Increase variance of $x$"
      ],
      "correctAnswer": "Increase variance of $x$",
      "difficulty": "medium",
      "topic": "Lecture 2 – Precision of OLS",
      "explanation": "$\\operatorname{Var}(\\hat\\beta_1)$ decreases with larger $\\operatorname{Var}(x)$ and larger $n$, but increases with $\\sigma_u^2$."
    },
    {
      "id": "mcq_010",
      "questionText": "In $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u$, $\\beta_1$ measures:",
      "options": [
        "Total effect of $x_1$ ignoring $x_2$",
        "Correlation of $x_1$ and $y$",
        "Effect of $x_1$ on $y$ holding $x_2$ constant",
        "Average of $y$ when $x_1=1$"
      ],
      "correctAnswer": "Effect of $x_1$ on $y$ holding $x_2$ constant",
      "difficulty": "easy",
      "topic": "Lecture 2 – Interpretation",
      "explanation": "$\\beta_1$ is the ceteris paribus (partial) effect: $\\partial y/\\partial x_1$ given $x_2$."
    },
    {
      "id": "mcq_011",
      "questionText": "To test that several coefficients are jointly zero, use:",
      "options": [
        "t-test",
        "F-test",
        "Durbin–Watson test",
        "Breusch–Pagan test"
      ],
      "correctAnswer": "F-test",
      "difficulty": "easy",
      "topic": "Lecture 2 – Hypothesis Testing",
      "explanation": "The F-test evaluates multiple linear restrictions $H_0: R\\beta=r$. A t-test is for a single linear restriction."
    },
    {
      "id": "mcq_012",
      "questionText": "High (not perfect) multicollinearity primarily causes:",
      "options": [
        "Biased coefficients",
        "Large standard errors",
        "Lower $R^2$",
        "Biased intercept only"
      ],
      "correctAnswer": "Large standard errors",
      "difficulty": "medium",
      "topic": "Lecture 2 – Multicollinearity",
      "explanation": "Near collinearity inflates variances of $\\hat\\beta$ and makes estimates unstable across similar specifications; it does not bias OLS if $E[u|X]=0$."
    },
    {
      "id": "mcq_013",
      "questionText": "Consider the regression: $\\text{Wage} = \\beta_0 + \\beta_1 \\cdot \\text{Female} + \\beta_2 \\cdot \\text{Educ} + \\beta_3 \\cdot (\\text{Female} \\times \\text{Educ}) + u$. What does $\\beta_3$ measure?",
      "options": [
        "Wage gap at zero education",
        "Return to education for females",
        "Difference in returns to education between females and males",
        "Return to education for males"
      ],
      "correctAnswer": "Difference in returns to education between females and males",
      "difficulty": "medium",
      "topic": "Lecture 2 – Interactions and Dummies",
      "explanation": "$\\beta_3$ is the interaction slope difference: $\\partial\\text{Wage}/\\partial\\text{Educ}$ for females minus that for males."
    },
    {
      "id": "mcq_014",
      "questionText": "Adjusted $R^2$ compared to $R^2$:",
      "options": [
        "Always increases when adding regressors",
        "Penalizes extra regressors and can fall when adding irrelevant variables",
        "Ignores sample size",
        "Is numerically identical to AIC"
      ],
      "correctAnswer": "Penalizes extra regressors and can fall when adding irrelevant variables",
      "difficulty": "medium",
      "topic": "Lecture 2 – Model Fit",
      "explanation": "$R^2_{adj}=1-(1-R^2)\\frac{n-1}{n-k-1}$; adding weak regressors can reduce $R^2_{adj}$ due to the degrees-of-freedom penalty."
    },
    {
      "id": "mcq_015",
      "questionText": "AIC vs BIC: typically, BIC:",
      "options": [
        "Penalizes less and selects larger models",
        "Penalizes more and selects simpler models",
        "Ignores likelihood",
        "Equals adjusted $R^2$"
      ],
      "correctAnswer": "Penalizes more and selects simpler models",
      "difficulty": "medium",
      "topic": "Lecture 3 – Model Selection",
      "explanation": "BIC penalty is $k\\ln n$ (larger than AIC’s $2k$ for $n\\ge 8$), so BIC tends to choose more parsimonious models."
    },
    {
      "id": "mcq_016",
      "questionText": "A time series is covariance stationary if:",
      "options": [
        "Mean, variance, and autocovariance are time-invariant",
        "Mean and variance are zero",
        "It has a deterministic trend only",
        "Its autocorrelations are all zero"
      ],
      "correctAnswer": "Mean, variance, and autocovariance are time-invariant",
      "difficulty": "medium",
      "topic": "Lecture 3 – Stationarity",
      "explanation": "Weak stationarity: $E(Y_t)=\\mu$, $\\operatorname{Var}(Y_t)=\\sigma^2$, and $\\operatorname{Cov}(Y_t,Y_{t-k})=\\gamma_k$ depends only on $k$."
    },
    {
      "id": "mcq_017",
      "questionText": "ADF test is primarily used to:",
      "options": [
        "Detect serial correlation in residuals",
        "Test for a unit root",
        "Test for heteroskedasticity",
        "Select lag length"
      ],
      "correctAnswer": "Test for a unit root",
      "difficulty": "medium",
      "topic": "Lecture 3 – Unit Root Tests",
      "explanation": "ADF tests $H_0:$ unit root via $\\Delta y_t=\\alpha+\\beta t+\\phi y_{t-1}+\\sum_{i=1}^p\\delta_i\\Delta y_{t-i}+\\varepsilon_t$; reject if $\\hat\\phi$ is sufficiently negative."
    },
    {
      "id": "mcq_018",
      "questionText": "Regressing two unrelated trending series on each other with high $R^2$ indicates:",
      "options": [
        "Omitted variable bias",
        "Spurious regression",
        "Measurement error",
        "Perfect multicollinearity"
      ],
      "correctAnswer": "Spurious regression",
      "difficulty": "medium",
      "topic": "Lecture 3 – Spurious Regression",
      "explanation": "Non-stationary trends can produce high $R^2$ and significant $t$-stats even without a true relationship; differencing or cointegration methods are needed."
    },
    {
      "id": "mcq_019",
      "questionText": "In AR(1): $Y_t = \\alpha + \\phi Y_{t-1} + u_t$, $|\\phi|=1$ implies:",
      "options": [
        "Stationary mean reversion",
        "Explosive series",
        "Random walk (non-stationary)",
        "White noise"
      ],
      "correctAnswer": "Random walk (non-stationary)",
      "difficulty": "medium",
      "topic": "Lecture 3 – AR(1)",
      "explanation": "When $|\\phi|=1$, the process has a unit root; shocks are permanent and variance increases with $t$."
    },
    {
      "id": "mcq_020",
      "questionText": "White noise errors $u_t$ are characterized by:",
      "options": [
        "Nonzero mean and no autocorrelation",
        "Zero mean, constant variance, no autocorrelation",
        "Time-varying variance and autocorrelation",
        "Strong seasonality"
      ],
      "correctAnswer": "Zero mean, constant variance, no autocorrelation",
      "difficulty": "easy",
      "topic": "Lecture 3 – White Noise",
      "explanation": "White noise: $E(u_t)=0$, $\\operatorname{Var}(u_t)=\\sigma^2$, and $\\operatorname{Cov}(u_t,u_s)=0$ for $t\\ne s$."
    },
    {
      "id": "mcq_021",
      "questionText": "A correlogram (ACF plot) shows bars outside confidence bands at low lags. This suggests:",
      "options": [
        "No persistence",
        "Statistically significant autocorrelation at those lags",
        "Heteroskedasticity",
        "A unit root is certain"
      ],
      "correctAnswer": "Statistically significant autocorrelation at those lags",
      "difficulty": "medium",
      "topic": "Lecture 3 – ACF/Correlogram",
      "explanation": "Bars beyond the bands indicate $\\rho_k\\ne 0$; the series exhibits serial dependence at those lags."
    },
    {
      "id": "mcq_022",
      "questionText": "Breusch–Godfrey test is used to detect:",
      "options": [
        "Heteroskedasticity",
        "Serial correlation up to order p in residuals",
        "Unit roots in $Y$",
        "Nonlinearity"
      ],
      "correctAnswer": "Serial correlation up to order p in residuals",
      "difficulty": "medium",
      "topic": "Lecture 3 – Diagnostics",
      "explanation": "BG tests $H_0:$ no AR($p$) in residuals by regressing $\\hat u_t$ on the original regressors and $\\hat u_{t-1},\\ldots,\\hat u_{t-p}$."
    },
    {
      "id": "mcq_023",
      "questionText": "In an MA($q$) model, for horizon $h>q$, forecasts tend to:",
      "options": [
        "Explode",
        "Follow the last value",
        "Revert to the process mean",
        "Oscillate randomly around zero with growing variance"
      ],
      "correctAnswer": "Revert to the process mean",
      "difficulty": "medium",
      "topic": "Lecture 3 – Forecasting",
      "explanation": "For $h>q$, future shocks are unknown with zero conditional mean, so forecasts converge to $\\mu$."
    },
    {
      "id": "mcq_024",
      "questionText": "In an ARDL($p,q$) model, the long-run multiplier for $X$ represents:",
      "options": [
        "Immediate impact of $X$ on $Y$",
        "Total equilibrium change in $Y$ after a permanent change in $X$",
        "Persistence of $Y$ via AR terms",
        "Average short-run effect"
      ],
      "correctAnswer": "Total equilibrium change in $Y$ after a permanent change in $X$",
      "difficulty": "medium",
      "topic": "Lecture 3 – ARDL",
      "explanation": "LRM $=\\dfrac{\\sum_{j=0}^{q} \\beta_j}{1-\\sum_{i=1}^{p} \\phi_i}$ is the total long-run response after full adjustment."
    },
    {
      "id": "mcq_025",
      "questionText": "Comparing two forecasting models using RMSE, a lower RMSE indicates:",
      "options": [
        "Better in-sample fit necessarily",
        "Smaller average forecast error magnitude",
        "Lower bias but higher variance",
        "Greater parsimony"
      ],
      "correctAnswer": "Smaller average forecast error magnitude",
      "difficulty": "easy",
      "topic": "Lecture 3 – Forecast Evaluation",
      "explanation": "RMSE $=\\sqrt{\\tfrac{1}{n}\\sum e_t^2}$; lower RMSE means, on average, smaller squared forecast errors (out-of-sample)."
    },
    {
      "id": "mcq_026",
      "questionText": "Including a linear time trend $t$ in a regression primarily captures:",
      "options": [
        "Short-run fluctuations",
        "Long-run deterministic growth/decline",
        "Seasonal effects",
        "Autocorrelated errors"
      ],
      "correctAnswer": "Long-run deterministic growth/decline",
      "difficulty": "medium",
      "topic": "Lecture 3 – Time Trends",
      "explanation": "A linear trend absorbs smooth long-run drift; seasonality is handled by seasonal dummies; AR terms address autocorrelation."
    },
    {
      "id": "mcq_027",
      "questionText": "Fixed Effects (Within) estimator removes unobserved time-invariant heterogeneity by:",
      "options": [
        "Differencing adjacent periods",
        "Subtracting each individual’s time mean",
        "Adding only time dummies",
        "Using cross-sectional averages"
      ],
      "correctAnswer": "Subtracting each individual’s time mean",
      "difficulty": "easy",
      "topic": "Lecture 4 – Fixed Effects",
      "explanation": "Within transformation: $y_{it}-\\bar y_i$ and $x_{it}-\\bar x_i$ eliminate unit fixed effects $\\alpha_i$."
    },
    {
      "id": "mcq_028",
      "questionText": "Which coefficient cannot be identified in a standard Fixed Effects model?",
      "options": [
        "A time-invariant characteristic (e.g., gender) for an individual",
        "A time-varying policy indicator",
        "Lagged dependent variable",
        "Time dummies"
      ],
      "correctAnswer": "A time-invariant characteristic (e.g., gender) for an individual",
      "difficulty": "easy",
      "topic": "Lecture 4 – FE Limitations",
      "explanation": "Time-invariant regressors are collinear with unit dummies and drop out in the within transformation."
    },
    {
      "id": "mcq_029",
      "questionText": "Strict exogeneity in panels requires that regressors are uncorrelated with:",
      "options": [
        "Only current errors",
        "Past and current errors",
        "All past, current, and future idiosyncratic errors",
        "Only future errors"
      ],
      "correctAnswer": "All past, current, and future idiosyncratic errors",
      "difficulty": "hard",
      "topic": "Lecture 4 – Exogeneity",
      "explanation": "Strict exogeneity: $E(u_{it}|X_i)=0$ for all $t$; this rules out feedback from $u$ to future $x$ and vice versa."
    },
    {
      "id": "mcq_030",
      "questionText": "FE vs First Differences: FD is preferred over FE when idiosyncratic errors:",
      "options": [
        "Are serially uncorrelated (white noise)",
        "Follow a random walk",
        "Are homoskedastic",
        "Are independent across individuals"
      ],
      "correctAnswer": "Follow a random walk",
      "difficulty": "hard",
      "topic": "Lecture 4 – FE vs FD",
      "explanation": "When $u_{it}$ is highly persistent (e.g., AR root near 1), first differencing can better remove it; FE is more efficient when $u_{it}$ is white noise."
    },
    {
      "id": "mcq_031",
      "questionText": "A balanced panel means:",
      "options": [
        "All individuals observed for all time periods",
        "No missing variables",
        "Equal numbers of individuals and periods",
        "Identical values of regressors across time"
      ],
      "correctAnswer": "All individuals observed for all time periods",
      "difficulty": "easy",
      "topic": "Lecture 4 – Panel Structure",
      "explanation": "Balanced panels have the same $T$ for each unit; unbalanced panels allow different $T_i$."
    },
    {
      "id": "mcq_032",
      "questionText": "Within vs Between variation: Fixed Effects estimates rely primarily on:",
      "options": [
        "Between-individual differences only",
        "Within-individual changes over time",
        "Cross-sectional averages",
        "Time dummies only"
      ],
      "correctAnswer": "Within-individual changes over time",
      "difficulty": "easy",
      "topic": "Lecture 4 – Within vs Between",
      "explanation": "FE identifies from deviations around each unit’s mean (within variation), not from cross-sectional differences."
    },
    {
      "id": "mcq_101",
      "questionText": "Which statement best distinguishes prediction from causal inference in econometrics?",
      "options": [
        "Prediction requires randomized experiments; causation does not",
        "Prediction optimizes forecast accuracy; causal inference seeks unbiased effect estimates",
        "Causal inference ignores identification; prediction focuses on it",
        "Both require the same assumptions"
      ],
      "correctAnswer": "Prediction optimizes forecast accuracy; causal inference seeks unbiased effect estimates",
      "difficulty": "easy",
      "topic": "Lecture 1 – Prediction vs Causation",
      "explanation": "Prediction targets low out-of-sample loss; causality targets unbiased effects under identification assumptions (e.g., conditional independence). Randomization aids causal identification, not prediction per se."
    },
    {
      "id": "mcq_102",
      "questionText": "Which dataset is time series data?",
      "options": [
        "500 households in 2024",
        "Monthly CPI for the Netherlands, 2010–2025",
        "Graduates surveyed in 2018 and a new sample in 2024",
        "2,000 firms tracked annually from 2015–2025"
      ],
      "correctAnswer": "Monthly CPI for the Netherlands, 2010–2025",
      "difficulty": "easy",
      "topic": "Lecture 1 – Data Types",
      "explanation": "Time series: repeated measures of the same variable over time. Panels track the same units; pooled cross-sections change units each wave."
    },
    {
      "id": "mcq_103",
      "questionText": "In simple OLS, which is guaranteed by the normal equations?",
      "options": [
        "Residuals sum to zero",
        "Residuals are homoskedastic",
        "Residuals are i.i.d.",
        "Regressors are exogenous"
      ],
      "correctAnswer": "Residuals sum to zero",
      "difficulty": "easy",
      "topic": "Lecture 1 – OLS Properties",
      "explanation": "With an intercept, $\\sum_i \\hat u_i=0$ exactly. Exogeneity and homoskedasticity are assumptions."
    },
    {
      "id": "mcq_104",
      "questionText": "Level–log model: $y = a + \\beta\\,\\log(x)$. If $\\beta = 2.5$, a 1% increase in $x$ is associated with:",
      "options": [
        "A 2.5% increase in $y$",
        "A 0.025 increase in $y$",
        "A 0.25 increase in $y$",
        "A 2.5 unit increase in $y$"
      ],
      "correctAnswer": "A 0.025 increase in $y$",
      "difficulty": "medium",
      "topic": "Lecture 1 – Functional Forms",
      "explanation": "Level–log: $\\partial y/\\partial\\ln x=\\beta$. A 1% change in $x$ is $\\Delta\\ln x\\approx 0.01$, so $\\Delta y\\approx \\beta\\times0.01=0.025$."
    },
    {
      "id": "mcq_105",
      "questionText": "OVB: $X$ (education) negatively correlated with omitted $Z$ (local unemployment risk), and $Z$ positively affects $Y$ (wages). Direction of bias on education?",
      "options": [
        "Upward",
        "Downward",
        "Zero",
        "Ambiguous"
      ],
      "correctAnswer": "Downward",
      "difficulty": "medium",
      "topic": "Lecture 2 – OVB Direction",
      "explanation": "Bias sign is $\\operatorname{sign}(\\beta_Z)\\times\\operatorname{sign}(\\operatorname{Cov}(X,Z))=(+)\\times(-)=-$ (downward)."
    },
    {
      "id": "mcq_106",
      "questionText": "Which is a symptom of high (but not perfect) multicollinearity?",
      "options": [
        "Large standard errors and unstable coefficient signs across similar specifications",
        "Biased OLS coefficients",
        "Lower RMSE out of sample",
        "Perfect prediction of $y$"
      ],
      "correctAnswer": "Large standard errors and unstable coefficient signs across similar specifications",
      "difficulty": "medium",
      "topic": "Lecture 2 – Multicollinearity",
      "explanation": "Near collinearity inflates $\\operatorname{Var}(\\hat\\beta)$ and makes signs sensitive; it does not bias OLS if $E[u|X]=0$."
    },
    {
      "id": "mcq_107",
      "questionText": "In a model with $k$ restrictions tested jointly, which statement is correct?",
      "options": [
        "For $k=1$, the F-test equals the squared t-test",
        "For any $k$, the F-test equals the squared t-test",
        "The F-test cannot be used when $k=1$",
        "The t-test is always more powerful than the F-test"
      ],
      "correctAnswer": "For $k=1$, the F-test equals the squared t-test",
      "difficulty": "medium",
      "topic": "Lecture 2 – Hypothesis Testing",
      "explanation": "When $k=1$, $F=t^2$. For $k>1$, use the F-test to assess joint restrictions."
    },
    {
      "id": "mcq_108",
      "questionText": "Categorical variable with 5 groups. How many dummies to include with an intercept?",
      "options": [
        "5",
        "4",
        "3",
        "2"
      ],
      "correctAnswer": "4",
      "difficulty": "easy",
      "topic": "Lecture 2 – Dummies",
      "explanation": "Include $G-1$ dummies to avoid perfect multicollinearity (here $5-1=4$)."
    },
    {
      "id": "mcq_109",
      "questionText": "Consider the regression model: $\\text{Wage} = \\beta_0 + \\beta_1 \\cdot \\text{Female} + \\beta_2 \\cdot \\text{Educ} + \\beta_3 \\cdot (\\text{Female} \\times \\text{Educ}) + u$. What is the return to education for females?",
      "options": [
        "$\\beta_2$",
        "$\\beta_2 + \\beta_3$",
        "$\\beta_1 + \\beta_2$",
        "$\\beta_0 + \\beta_2 + \\beta_3$"
      ],
      "correctAnswer": "$\\beta_2 + \\beta_3$",
      "difficulty": "medium",
      "topic": "Lecture 2 – Interactions",
      "explanation": "For females ($\\text{Female}=1$), the slope on Educ is $\\beta_2 + \\beta_3$."
    },
    {
      "id": "mcq_110",
      "questionText": "AIC/BIC are used primarily to:",
      "options": [
        "Test individual coefficients",
        "Balance fit and complexity for model selection",
        "Compute confidence intervals",
        "Detect heteroskedasticity"
      ],
      "correctAnswer": "Balance fit and complexity for model selection",
      "difficulty": "easy",
      "topic": "Lecture 3 – Model Selection",
      "explanation": "They trade off likelihood (fit) against a complexity penalty, discouraging overfitting."
    },
    {
      "id": "mcq_111",
      "questionText": "Random walk: for $X_t = X_{t-1} + u_t$ with $\\operatorname{Var}(u_t)=\\sigma^2$ and $X_0=0$, what is $\\operatorname{Var}(X_t)$?",
      "options": [
        "$\\sigma^2$",
        "$t\\sigma^2$",
        "$\\sigma^2/t$",
        "$2\\sigma^2$"
      ],
      "correctAnswer": "$t\\sigma^2$",
      "difficulty": "medium",
      "topic": "Lecture 3 – Random Walk Variance",
      "explanation": "$X_t=\\sum_{i=1}^t u_i$ with independent shocks $\\Rightarrow \\operatorname{Var}(X_t)=\\sum\\sigma^2=t\\sigma^2$."
    },
    {
      "id": "mcq_112",
      "questionText": "ACF patterns: which statement is most typical?",
      "options": [
        "AR(1) ACF cuts off after lag 1; MA(1) ACF decays geometrically",
        "AR(1) ACF decays geometrically; MA(1) ACF cuts off after lag 1",
        "Both ACFs cut off after lag 1",
        "Both ACFs decay geometrically"
      ],
      "correctAnswer": "AR(1) ACF decays geometrically; MA(1) ACF cuts off after lag 1",
      "difficulty": "medium",
      "topic": "Lecture 3 – ACF Interpretation",
      "explanation": "AR processes show gradually decaying ACF; MA($q$) processes have ACF that becomes zero beyond lag $q$."
    },
    {
      "id": "mcq_113",
      "questionText": "ADF including a constant yields p = 0.03. Using 5% significance, conclude:",
      "options": [
        "Fail to reject unit root; series is non-stationary",
        "Reject unit root; series is stationary around a constant",
        "Series is explosive",
        "Series is trend-stationary by definition"
      ],
      "correctAnswer": "Reject unit root; series is stationary around a constant",
      "difficulty": "easy",
      "topic": "Lecture 3 – ADF Decision",
      "explanation": "p = 0.03 $<$ 0.05: reject $H_0$ of a unit root in the constant-only specification $\\Rightarrow$ stationary mean."
    },
    {
      "id": "mcq_114",
      "questionText": "BG test auxiliary regression includes:",
      "options": [
        "Only lagged residuals",
        "Original regressors and lagged residuals",
        "Only the dependent variable and its lag",
        "Only time dummies"
      ],
      "correctAnswer": "Original regressors and lagged residuals",
      "difficulty": "medium",
      "topic": "Lecture 3 – Breusch–Godfrey Procedure",
      "explanation": "Regress $\\hat u_t$ on the original $X_t$ and on $\\hat u_{t-1},...,\\hat u_{t-p}$; test joint significance of lagged residuals."
    },
    {
      "id": "mcq_115",
      "questionText": "AR(1): $y_t = \\alpha + \\phi y_{t-1} + u_t$, with $|\\phi|<1$. The long-run mean equals:",
      "options": [
        "0",
        "c",
        "$c/(1-\\phi)$",
        "$\\phi/(1-c)$"
      ],
      "correctAnswer": "$c/(1-\\phi)$",
      "difficulty": "medium",
      "topic": "Lecture 3 – AR(1) Mean",
      "explanation": "Steady state $\\mu$ solves $\\mu=\\alpha+\\phi\\mu \\Rightarrow \\mu=\\alpha/(1-\\phi)$."
    },
    {
      "id": "mcq_116",
      "questionText": "Forecast evaluation: MAPE can be problematic when:",
      "options": [
        "Errors are homoskedastic",
        "Actual values are close to zero",
        "Forecast horizon is short",
        "Data are in logs"
      ],
      "correctAnswer": "Actual values are close to zero",
      "difficulty": "medium",
      "topic": "Lecture 3 – Forecast Metrics",
      "explanation": "MAPE uses $|e_t|/|y_t|$; near-zero $|y_t|$ makes the ratio explode, distorting comparisons."
    },
    {
      "id": "mcq_117",
      "questionText": "ARDL intuition: the long-run multiplier divides the sum of $X$-lag coefficients by:",
      "options": [
        "$1-\\sum$ of $Y$-lag coefficients",
        "Sum of $Y$-lag coefficients",
        "Variance of residuals",
        "AIC-selected lag order"
      ],
      "correctAnswer": "$1-\\sum$ of $Y$-lag coefficients",
      "difficulty": "medium",
      "topic": "Lecture 3 – ARDL Long-Run",
      "explanation": "LRM $=\\dfrac{\\sum_j \\beta_j}{1-\\sum_i \\phi_i}$; denominator captures persistence due to lagged $y$."
    },
    {
      "id": "mcq_118",
      "questionText": "A model’s residual correlogram shows no significant autocorrelation at any lag. This suggests the model is:",
      "options": [
        "Overfit",
        "Dynamically complete (residuals \\~ white noise)",
        "Non-stationary",
        "Misspecified in levels"
      ],
      "correctAnswer": "Dynamically complete (residuals \\~ white noise)",
      "difficulty": "easy",
      "topic": "Lecture 3 – Model Diagnostics",
      "explanation": "If residuals resemble white noise, remaining dynamics are minimal and the model captures predictable structure."
    },
    {
      "id": "mcq_119",
      "questionText": "First Differences (FD) vs Fixed Effects (FE): when $T=2$, which is true?",
      "options": [
        "FD and FE are numerically identical for slope coefficients",
        "FD dominates FE in efficiency",
        "FE dominates FD in efficiency",
        "Neither can be estimated"
      ],
      "correctAnswer": "FD and FE are numerically identical for slope coefficients",
      "difficulty": "medium",
      "topic": "Lecture 4 – FE vs FD",
      "explanation": "With two periods, within-transformation equals differencing, yielding the same slope estimates."
    },
    {
      "id": "mcq_120",
      "questionText": "Including individual and time dummies in a panel regression primarily controls for:",
      "options": [
        "Time-invariant individual heterogeneity and common shocks over time",
        "Only observed covariates",
        "Measurement error",
        "Serial correlation"
      ],
      "correctAnswer": "Time-invariant individual heterogeneity and common shocks over time",
      "difficulty": "easy",
      "topic": "Lecture 4 – Two-Way FE (time dummies)",
      "explanation": "Unit FE absorb $\\alpha_i$; time FE (dummies) absorb $\\lambda_t$ that shift all units in a period."
    },
    {
      "id": "mcq_121",
      "questionText": "Strict exogeneity fails in FE when the model includes:",
      "options": [
        "Time dummies",
        "Lagged dependent variable",
        "Heteroskedastic errors",
        "Individual dummies"
      ],
      "correctAnswer": "Lagged dependent variable",
      "difficulty": "hard",
      "topic": "Lecture 4 – Exogeneity",
      "explanation": "Including $y_{i,t-1}$ generates correlation between transformed regressors and transformed errors (Nickell bias) when $T$ is small."
    },
    {
      "id": "mcq_122",
      "questionText": "Unbalanced panels: which statement is correct?",
      "options": [
        "FE cannot be estimated with unbalanced panels",
        "FE can be estimated; observations with missing years are used where available",
        "Unbalanced panels imply inconsistent OLS",
        "Only FD works with unbalanced panels"
      ],
      "correctAnswer": "FE can be estimated; observations with missing years are used where available",
      "difficulty": "medium",
      "topic": "Lecture 4 – Panel Structure",
      "explanation": "FE estimation accommodates differing $T_i$; within variation is exploited as available per unit."
    },
    {
      "id": "mcq_123",
      "questionText": "If residuals in a time-series regression show heteroskedasticity and autocorrelation, a common remedy for inference is to use:",
      "options": [
        "Classical OLS standard errors",
        "HAC/Newey–West standard errors",
        "Cluster by individual",
        "Jackknife standard errors"
      ],
      "correctAnswer": "HAC/Newey–West standard errors",
      "difficulty": "medium",
      "topic": "Lecture 3 – Robust Inference",
      "explanation": "HAC SEs are consistent under unknown heteroskedasticity and autocorrelation in $u_t$; ordinary OLS SEs are invalid in this case."
    },
    {
      "id": "mcq_124",
      "questionText": "Distributed lag model with $\\beta_0=0.4$ and $\\beta_1=0.3$ (only $X_t$ and $X_{t-1}$). The long-run (total) multiplier equals:",
      "options": [
        "$0.4$",
        "$0.3$",
        "$0.7$",
        "$1.0$"
      ],
      "correctAnswer": "$0.7$",
      "difficulty": "easy",
      "topic": "Lecture 3 – DL Multipliers",
      "explanation": "The long-run (total) effect is the sum of DL coefficients: $0.4+0.3=0.7$."
    },
    {
      "id": "mcq_125",
      "questionText": "ARMA(1,1) forecasts: which statement is most accurate?",
      "options": [
        "At $h=2$, the MA component still enters via future shocks",
        "At $h=2$, the MA component drops out because future shocks have zero conditional mean",
        "At $h=1$, only the MA component matters",
        "ARMA forecasts revert immediately to the mean at $h=1$"
      ],
      "correctAnswer": "At $h=2$, the MA component drops out because future shocks have zero conditional mean",
      "difficulty": "medium",
      "topic": "Lecture 3 – ARMA Forecasting",
      "explanation": "For $h\\ge2$, $E(\\varepsilon_{t+j}|I_t)=0$ eliminates MA terms; multi-step forecasts are driven by the AR component."
    },
    {
      "id": "mcq_126",
      "questionText": "Breusch–Pagan (BP) vs White test for heteroskedasticity: which statement is most accurate?",
      "options": [
        "BP allows a very general, fully nonparametric form; White assumes a specific functional form",
        "White is more general because it uses squares and cross-products of regressors; BP typically regresses squared residuals on the original regressors",
        "Both are identical in all cases",
        "BP adjusts standard errors; White does not"
      ],
      "correctAnswer": "White is more general because it uses squares and cross-products of regressors; BP typically regresses squared residuals on the original regressors",
      "difficulty": "medium",
      "topic": "Tutorial – Heteroskedasticity Tests",
      "explanation": "BP tests whether $\\operatorname{Var}(u|X)$ varies linearly with $X$. White’s test augments with $X^2$ and cross-terms, capturing a broader class of variance patterns."
    },
    {
      "id": "mcq_127",
      "questionText": "When should you prefer cluster-robust standard errors over HC (heteroskedasticity-robust) SEs?",
      "options": [
        "When errors are i.i.d.",
        "When observations may be correlated within groups (e.g., firms, countries) but independent across groups",
        "Only in time series",
        "Only when there are exactly two clusters"
      ],
      "correctAnswer": "When observations may be correlated within groups (e.g., firms, countries) but independent across groups",
      "difficulty": "medium",
      "topic": "Tutorial – Robust vs Cluster SEs",
      "explanation": "Cluster SEs allow arbitrary intra-cluster correlation/heteroskedasticity; HC SEs assume independence across observations. You also need a sufficiently large number of clusters for reliable inference."
    },
    {
      "id": "mcq_128",
      "questionText": "What is a practical issue with cluster-robust SEs when the number of clusters is small?",
      "options": [
        "They become biased downward and inference can be anti-conservative",
        "They always explode to infinity",
        "They are identical to HAC SEs",
        "They are invalid in panels"
      ],
      "correctAnswer": "They become biased downward and inference can be anti-conservative",
      "difficulty": "hard",
      "topic": "Tutorial – Cluster SEs (Few Clusters)",
      "explanation": "With few clusters, standard cluster-robust variance estimators tend to understate true variability; small-sample corrections or wild cluster bootstrap are common remedies."
    },
    {
      "id": "mcq_129",
      "questionText": "HAC (Newey–West) vs Cluster SEs: which guidance is correct?",
      "options": [
        "Use HAC for cross-sectional clustering and Cluster for serial correlation",
        "Use HAC for serial correlation in a single time series; use Cluster when units are grouped and errors correlate within groups",
        "They are interchangeable",
        "Neither handles heteroskedasticity"
      ],
      "correctAnswer": "Use HAC for serial correlation in a single time series; use Cluster when units are grouped and errors correlate within groups",
      "difficulty": "medium",
      "topic": "Tutorial – Robust Inference",
      "explanation": "HAC handles heteroskedasticity and autocorrelation over time for a single series; cluster SEs handle within-group dependence across observations in panels or multi-level data."
    },
    {
      "id": "mcq_130",
      "questionText": "RESET test primarily detects:",
      "options": [
        "Serial correlation",
        "Functional form misspecification (e.g., omitted nonlinearities)",
        "Heteroskedasticity",
        "Unit roots"
      ],
      "correctAnswer": "Functional form misspecification (e.g., omitted nonlinearities)",
      "difficulty": "medium",
      "topic": "Tutorial – Model Specification",
      "explanation": "The Ramsey RESET test adds powers of fitted values (or regressors); rejection suggests missing nonlinear terms or interactions."
    },
    {
      "id": "mcq_131",
      "questionText": "Leverage vs influence: which statement is true?",
      "options": [
        "Leverage measures how unusual $y$ is given $X$; influence measures how unusual $X$ is",
        "Leverage captures how extreme an observation's $X$ values are; influence reflects the observation’s impact on fitted coefficients (e.g., Cook’s $D$)",
        "They are identical",
        "Only influence is used in practice"
      ],
      "correctAnswer": "Leverage captures how extreme an observation's $X$ values are; influence reflects the observation’s impact on fitted coefficients (e.g., Cook’s $D$)",
      "difficulty": "medium",
      "topic": "Tutorial – Diagnostics",
      "explanation": "High leverage points have extreme predictor values; high influence points substantially change estimates if removed (often high leverage and large residual)."
    },
    {
      "id": "mcq_132",
      "questionText": "Variance Inflation Factor (VIF) primarily indicates:",
      "options": [
        "Heteroskedasticity",
        "Endogeneity",
        "Multicollinearity severity for a regressor",
        "Autocorrelation"
      ],
      "correctAnswer": "Multicollinearity severity for a regressor",
      "difficulty": "easy",
      "topic": "Tutorial – Multicollinearity",
      "explanation": "VIF for $x_j$ equals $1/(1-R_j^2)$ where $R_j^2$ is from regressing $x_j$ on the other regressors; larger VIF indicates higher collinearity and larger $\\operatorname{Var}(\\hat\\beta_j)$."
    },
    {
      "id": "mcq_133",
      "questionText": "Choosing ADF specification: when should you include a time trend $t$ in the ADF regression?",
      "options": [
        "Always include a trend",
        "Never include a trend",
        "Include a trend when the series exhibits a clear deterministic trend (after visual inspection)",
        "Include a trend only if the ACF is flat"
      ],
      "correctAnswer": "Include a trend when the series exhibits a clear deterministic trend (after visual inspection)",
      "difficulty": "medium",
      "topic": "Tutorial – ADF Specification",
      "explanation": "ADF can be run with none/constant/trend. Visual inspection and context guide whether a deterministic trend is plausible; including unnecessary trend terms reduces power."
    },
    {
      "id": "mcq_134",
      "questionText": "Lag length in ADF tests is often chosen by:",
      "options": [
        "Maximizing $R^2$",
        "Minimizing AIC/BIC or ensuring white-noise residuals via BG tests",
        "Setting it to 1 always",
        "Trial-and-error without diagnostics"
      ],
      "correctAnswer": "Minimizing AIC/BIC or ensuring white-noise residuals via BG tests",
      "difficulty": "medium",
      "topic": "Tutorial – ADF Lag Selection",
      "explanation": "Include enough lagged differences to remove serial correlation in residuals (checked by BG/Ljung–Box) or use information criteria to balance fit and parsimony."
    },
    {
      "id": "mcq_135",
      "questionText": "Out-of-sample forecast evaluation: which split procedure best mimics real-time forecasting?",
      "options": [
        "Random train/test split",
        "K-fold cross-validation",
        "Rolling or expanding window evaluation by time",
        "Leave-one-out cross-validation"
      ],
      "correctAnswer": "Rolling or expanding window evaluation by time",
      "difficulty": "medium",
      "topic": "Tutorial – Forecast Evaluation",
      "explanation": "Forecasts should respect temporal ordering; rolling/expanding evaluation uses past to predict future, avoiding look-ahead bias inherent in random splits."
    },
    {
      "id": "mcq_136",
      "questionText": "Prediction intervals vs confidence intervals: which statement is correct?",
      "options": [
        "They are identical in regression",
        "Prediction intervals are wider because they include both parameter uncertainty and future shock variance",
        "Confidence intervals are wider because they include future shock variance",
        "Neither involves the error variance"
      ],
      "correctAnswer": "Prediction intervals are wider because they include both parameter uncertainty and future shock variance",
      "difficulty": "medium",
      "topic": "Tutorial – Forecast Uncertainty",
      "explanation": "A confidence interval is for $E[y|X]$; a prediction interval adds $\\operatorname{Var}(\\varepsilon_{t+h})$, so it is wider."
    },
    {
      "id": "mcq_137",
      "questionText": "Seasonality handling: which approach is most appropriate in quarterly data with stable seasonal patterns?",
      "options": [
        "Add quarterly dummies (Q1–Q4) to the regression",
        "Always difference the series 4 times",
        "Only include a linear trend",
        "Use Breusch–Pagan test"
      ],
      "correctAnswer": "Add quarterly dummies (Q1–Q4) to the regression",
      "difficulty": "easy",
      "topic": "Tutorial – Seasonality",
      "explanation": "Seasonal dummies capture deterministic seasonal effects; seasonal differencing may be needed for stochastic seasonality but is not automatic."
    },
    {
      "id": "mcq_138",
      "questionText": "Durbin–Watson (DW) vs Breusch–Godfrey (BG): why is BG generally preferred in multiple regression?",
      "options": [
        "DW handles AR(p) while BG only AR(1)",
        "BG allows testing higher-order serial correlation and accommodates lagged dependent variables among regressors",
        "DW is valid with lagged dependent variables",
        "They are identical"
      ],
      "correctAnswer": "BG allows testing higher-order serial correlation and accommodates lagged dependent variables among regressors",
      "difficulty": "medium",
      "topic": "Tutorial – Serial Correlation Tests",
      "explanation": "DW is primarily for AR(1) and breaks down with lagged dependent variables; BG’s auxiliary regression handles AR(p) and keeps original regressors."
    },
    {
      "id": "mcq_139",
      "questionText": "Time-invariant variables in FE: how can you estimate their effects?",
      "options": [
        "You cannot under any circumstance",
        "Interact them with time dummies or use a model that retains between variation (e.g., RE or correlated RE)",
        "Difference them",
        "Add more lags"
      ],
      "correctAnswer": "Interact them with time dummies or use a model that retains between variation (e.g., RE or correlated RE)",
      "difficulty": "hard",
      "topic": "Tutorial – FE Limitations and Workarounds",
      "explanation": "Pure FE drops time-invariant regressors; interactions with time or alternative estimators (RE/CRE) leverage between variation to identify such effects."
    },
    {
      "id": "mcq_140",
      "questionText": "Dynamic panels with FE and small $T$: what is the main concern?",
      "options": [
        "Heteroskedasticity only",
        "Nickell bias in the coefficient on lagged $y$",
        "Perfect multicollinearity",
        "Unit roots in $x$"
      ],
      "correctAnswer": "Nickell bias in the coefficient on lagged $y$",
      "difficulty": "hard",
      "topic": "Tutorial – Dynamic Panels",
      "explanation": "With small $T$, within-OLS on $y_{i,t-1}$ is biased (Nickell, 1981). Arellano–Bond type GMM or bias corrections are typical remedies."
    },
    {
      "id": "mcq_141",
      "questionText": "In panel data, which test compares Fixed Effects (FE) and Random Effects (RE) consistency assumptions?",
      "options": [
        "Breusch–Pagan LM test",
        "Hausman test",
        "F-test for individual effects",
        "Wald test"
      ],
      "correctAnswer": "Hausman test",
      "difficulty": "medium",
      "topic": "Tutorial – FE vs RE Testing",
      "explanation": "The Hausman test checks whether $E(u_i|X_i)=0$. If violated, RE is inconsistent, and FE is preferred. It compares FE and RE coefficient estimates directly."
    },
    {
      "id": "mcq_142",
      "questionText": "What is the consequence of serial correlation in panel FE model residuals if ignored?",
      "options": [
        "Coefficients become biased",
        "Standard errors are underestimated and inference becomes invalid",
        "Within transformation fails",
        "Fixed effects disappear"
      ],
      "correctAnswer": "Standard errors are underestimated and inference becomes invalid",
      "difficulty": "medium",
      "topic": "Tutorial – Panel Diagnostics",
      "explanation": "Serially correlated residuals violate classical assumptions, making conventional SEs too small. Clustered or robust SEs should be used to restore valid inference."
    },
    {
      "id": "mcq_143",
      "questionText": "When interpreting FE coefficients, what does it mean if an estimated coefficient is based on little within variation?",
      "options": [
        "Estimate is unbiased but highly precise",
        "Estimate is unreliable because of weak within variation for that regressor",
        "Estimate represents between variation only",
        "Estimate automatically equals OLS result"
      ],
      "correctAnswer": "Estimate is unreliable because of weak within variation for that regressor",
      "difficulty": "medium",
      "topic": "Tutorial – FE Interpretation",
      "explanation": "FE relies solely on within-unit changes. If a regressor barely varies within units, its estimate becomes imprecise (large standard error) and possibly unstable."
    },
    {
      "id": "mcq_144",
      "questionText": "In a difference-in-differences (DiD) design, the key identifying assumption is:",
      "options": [
        "Treatment and control groups have identical average outcomes before treatment",
        "Trends in outcomes would have been parallel absent treatment",
        "Random assignment of treatment",
        "No omitted variables"
      ],
      "correctAnswer": "Trends in outcomes would have been parallel absent treatment",
      "difficulty": "hard",
      "topic": "Tutorial – Difference-in-Differences",
      "explanation": "The DiD estimator attributes post–pre differences in treatment vs control groups to the treatment if their counterfactual trends would have been parallel in absence of treatment."
    },
    {
      "id": "mcq_145",
      "questionText": "In a DiD regression with unit and time fixed effects, the treatment coefficient captures:",
      "options": [
        "Raw difference in outcomes between groups",
        "Average treatment effect on treated (ATT), controlling for time and group effects",
        "Correlation between treatment and outcome",
        "Unit-specific time trends"
      ],
      "correctAnswer": "Average treatment effect on treated (ATT), controlling for time and group effects",
      "difficulty": "medium",
      "topic": "Tutorial – Difference-in-Differences",
      "explanation": "Including unit and time fixed effects controls for constant differences and common trends. The DiD coefficient on the treatment dummy identifies the ATT under parallel trends."
    },
    {
      "id": "mcq_146",
      "questionText": "Which scenario violates strict exogeneity in a panel FE regression $y_{it} = x_{it}'\\beta + \\alpha_i + u_{it}$?",
      "options": [
        "The firm sets $x_{i,t+1}$ in response to an unexpected cost shock $u_{it}$",
        "$x_{it}$ depends on $u_{i,t-1}$ and $u$ is serially uncorrelated",
        "An experiment randomly assigns $x_{it}$ in each period",
        "Adding time dummies controls for common shocks"
      ],
      "correctAnswer": "The firm sets $x_{i,t+1}$ in response to an unexpected cost shock $u_{it}$",
      "difficulty": "medium",
      "topic": "Lecture 4 – Exogeneity",
      "explanation": "Strict exogeneity requires $E(u_{it}\\,|\\,X_i)=0$ for all $t$, i.e., $u_{it}$ is uncorrelated with regressors from every period. Feedback from current shocks to future regressors (when $x_{i,t+1}$ depends on $u_{it}$) makes $u_{it}$ correlated with elements of $X_i$, violating strict exogeneity. Predeterminedness (only past $u$ affecting current $x$) with serially uncorrelated $u$ need not violate strict exogeneity; random assignment satisfies it; time dummies do not resolve unit-specific feedback."
    }
  ]
}